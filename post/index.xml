<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Statistical Genetics</title>
    <link>https://Hardy-Lab-Statistical-Genetics.github.io/post/</link>
    <description>Recent content in Posts on Statistical Genetics</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 11 Mar 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://Hardy-Lab-Statistical-Genetics.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How to parrallelize R functions with HLSGUtils package?</title>
      <link>https://Hardy-Lab-Statistical-Genetics.github.io/p/how-to-parrallelize-r-functions-with-hlsgutils-package/</link>
      <pubDate>Fri, 11 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://Hardy-Lab-Statistical-Genetics.github.io/p/how-to-parrallelize-r-functions-with-hlsgutils-package/</guid>
      <description>Suppose we want to run different linear models on a dataset. We can write it with a for loop in R, but as a result of many R functions working with one core, this process, It will eventually run on one core and is time-consuming. Another solution is to use multiprocess packages in R, like parallel. These packages are very good, but the time of the executions does not decrease linearly when we increase the computation cores.</description>
    </item>
    
    <item>
      <title>Fast and Scalable Genomics Workflows with Spark</title>
      <link>https://Hardy-Lab-Statistical-Genetics.github.io/p/fast-and-scalable-genomic-workflows-with-spark/</link>
      <pubDate>Mon, 07 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://Hardy-Lab-Statistical-Genetics.github.io/p/fast-and-scalable-genomic-workflows-with-spark/</guid>
      <description>Every seven months, the volume of genomics data doubles. As a result, scientists in this field face big data challenges such as data management and designing efficient algorithms. In this post, we will attempt to apply Glow solution to create big data architectures to genomics data. Glow is built on Apache Spark and Delta Lake, two popular big data technologies for distributed data processing and storage.
source: databricks.com</description>
    </item>
    
    <item>
      <title>Introduction to ADNI Dataset</title>
      <link>https://Hardy-Lab-Statistical-Genetics.github.io/p/introduction-to-adni-dataset/</link>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://Hardy-Lab-Statistical-Genetics.github.io/p/introduction-to-adni-dataset/</guid>
      <description>What is ADNI Dataset? The Alzheimer’s Disease Neuroimaging Initiative (ADNI) unites researchers with study data as they work to define the progression of Alzheimer’s disease (AD). ADNI researchers collect, validate and utilize data, including MRI and PET images, genetics, cognitive tests, CSF and blood biomarkers as predictors of the disease. Study resources and data from the North American ADNI study are available through this website, including Alzheimer’s disease patients, mild cognitive impairment subjects, and elderly controls.</description>
    </item>
    
  </channel>
</rss>
